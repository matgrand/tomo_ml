{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import Module, Linear, Conv2d, MaxPool2d, BatchNorm2d, ReLU, Sequential, ConvTranspose2d\n",
    "import scipy.io as sio\n",
    "from time import time, sleep\n",
    "import numpy as np\n",
    "from numpy import pi as π\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")  # adds seaborn style to charts, eg. grid\n",
    "plt.style.use(\"dark_background\")  # inverts colors to dark theme\n",
    "plt.rcParams['font.family'] = 'monospace'\n",
    "np.set_printoptions(precision=3) # set precision for printing numpy arrays\n",
    "import os\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "try: \n",
    "    JOBID = os.environ[\"SLURM_JOB_ID\"] # get job id from slurm, when training on cluster\n",
    "    DEV = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") # nvidia\n",
    "    HAS_SCREEN = False # for plotting or saving images\n",
    "except:\n",
    "    DEV = torch.device(\"mps\") # apple silicon\n",
    "    JOBID = \"local\"\n",
    "    HAS_SCREEN = True\n",
    "os.makedirs(f\"mg_data/{JOBID}\", exist_ok=True)\n",
    "\n",
    "# DEV = torch.device(\"cpu\") # cpu\n",
    "print(f'DEV: {DEV}')\n",
    "\n",
    "def to_tensor(x, dev=torch.device(\"cpu\")): return torch.tensor(x, dtype=torch.float32, device=dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "GRID_SIZE = 32 # size of the grid\n",
    "SXRV_SIZE = 21 #21 # number of vertical sensors (match with dataset)\n",
    "SXRH_SIZE = 23 #23 # number of horizontal sensors (match with dataset)\n",
    "INPUT_SIZE = SXRH_SIZE + SXRV_SIZE\n",
    "DATASET_SIZE = 10_000 # number of samples in the dataset\n",
    "TRAIN_DS_PATH = f\"data/sxr_ds_{DATASET_SIZE}.npz\"\n",
    "EVAL_DS_PATH = f\"data/sxr_ds_{DATASET_SIZE//10}.npz\"\n",
    "# TRAIN_DS_PATH = \"data/sxr_ds_100000.npz\"\n",
    "# EVAL_DS_PATH = \"data/sxr_ds_10000.npz\"\n",
    "BATCH_SIZE = 1 #128 # NOTE: batch size 1 works best\n",
    "LOAD_PRETRAINED = None\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = np.ones(EPOCHS) * 3e-4 # learning rate\n",
    "\n",
    "SAVE_DIR = f\"mg_data/{JOBID}/lin\" \n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "# copy the python training to the directory (for cluster) (for local, it fails silently)\n",
    "os.system(f\"cp train.py {SAVE_DIR}/train.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "class SXRDataset(Dataset):\n",
    "    def __init__(self, ds_path):\n",
    "        ds = np.load(ds_path)\n",
    "        # soft x-ray horizontal and vertical sensors\n",
    "        self.sxr = to_tensor(np.concatenate([ds['sxrh'], ds['sxrv']], axis=-1), DEV)\n",
    "        self.em = to_tensor(ds['emiss_lr'], DEV) # emissivities (NxN)\n",
    "        self.RR, self.ZZ, self.rr, self.zz = ds['RR'], ds['ZZ'], ds['rr'], ds['zz'] # grid coordinates\n",
    "        assert len(self.em) == len(self.sxr), f'length mismatch: {len(self.em)} vs {len(self.sxr)}'\n",
    "        assert self.sxr.shape[-1] == INPUT_SIZE, f'sxr size mismatch: {self.sxr.shape[-1]}'\n",
    "        assert self.rr.shape[0] == GRID_SIZE, f'grid size ({self.RR.shape[0]}) is wrong, match it with dataset generator'\n",
    "    def __len__(self): return len(self.sxr)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sxr[idx], self.em[idx]\n",
    "\n",
    "# test dataset\n",
    "ds = SXRDataset('data/sxr_ds_1000.npz')\n",
    "print(f'ds len: {len(ds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataset\n",
    "ds = SXRDataset(EVAL_DS_PATH)\n",
    "print(f\"Dataset length: {len(ds)}\")\n",
    "print(f\"Input shape: {ds[0][0].shape}\")\n",
    "print(f\"Output shape: {ds[0][1].shape}\")\n",
    "n_plot = 10\n",
    "print(len(ds))\n",
    "fig, axs = plt.subplots(2, n_plot, figsize=(3*n_plot, 5))\n",
    "for i, j in enumerate(np.random.randint(0, len(ds), n_plot)):\n",
    "    sxr, em = ds[j][0].cpu().numpy().squeeze(), ds[j][1].cpu().numpy().squeeze()\n",
    "    axs[0,i].contourf(ds.rr, ds.zz, em, 100, cmap=\"inferno\")\n",
    "    axs[0,i].contour(ds.rr, ds.zz, -em, 20, colors=\"black\", linestyles=\"dotted\")\n",
    "    axs[0,i].axis(\"off\")\n",
    "    axs[0,i].set_aspect(\"equal\")\n",
    "    #plot sxr\n",
    "    axs[1,i].plot(sxr)\n",
    "plt.show() if HAS_SCREEN else plt.savefig(f\"mg_data/{JOBID}/dataset.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation functions\n",
    "class Swish(Module): # custom trainable swish\n",
    "    def __init__(self, β=1.0): \n",
    "        super(Swish, self).__init__()\n",
    "        self.β = torch.nn.Parameter(torch.tensor(β), requires_grad=True)\n",
    "    def forward(self, x): \n",
    "        return x*torch.sigmoid(self.β*x)\n",
    "    def to(self, device): \n",
    "        self.β = self.β.to(device)\n",
    "        return super().to(device)\n",
    "\n",
    "# class Λ(Module): # relu, uncomment for relu activation\n",
    "#     def __init__(self): super(Λ, self).__init__()\n",
    "#     def forward(self, x): return torch.relu(x)\n",
    "\n",
    "class Λ(Module): # swish, uncomment for swish activation\n",
    "    def __init__(self): \n",
    "        super(Λ, self).__init__()\n",
    "        self.β = torch.nn.Parameter(torch.tensor(1.0), requires_grad=True)\n",
    "    def forward(self, x): return x*torch.sigmoid(self.β*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network architecture (1 layer fully connected)\n",
    "WIDTH = 1024*8\n",
    "class SXRNet(Module):\n",
    "    def __init__(self):\n",
    "        super(SXRNet, self).__init__()\n",
    "        self.net = Sequential(\n",
    "            Linear(INPUT_SIZE, WIDTH), Λ(),\n",
    "            Linear(WIDTH, GRID_SIZE*GRID_SIZE), Λ(),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        x = x.view(-1, 1, GRID_SIZE, GRID_SIZE)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model\n",
    "model = SXRNet()\n",
    "input = torch.randn(1, INPUT_SIZE)\n",
    "output = model(input)\n",
    "print(f'Input: {input.shape} Output: {output.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    train_ds, val_ds = SXRDataset(TRAIN_DS_PATH), SXRDataset(EVAL_DS_PATH) # initialize datasets\n",
    "    train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True) # initialize DataLoader\n",
    "    val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)  \n",
    "    model = SXRNet()  # instantiate model\n",
    "    if LOAD_PRETRAINED is not None: # load pretrained model\n",
    "        model.load_state_dict(torch.load(LOAD_PRETRAINED, map_location=torch.device(\"cpu\"))) # load pretrained model\n",
    "        print(f\"Pretrained model loaded: {LOAD_PRETRAINED}\")\n",
    "    model.to(DEV) # move model to DEV\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE[0])\n",
    "    loss_fn = torch.nn.MSELoss() # Mean Squared Error Loss\n",
    "    tlog_tot, elog_tot = [], []# logs for losses\n",
    "    start_time = time() # start time\n",
    "    for ep in range(EPOCHS): # epochs\n",
    "        epoch_time = time()\n",
    "        for pg in optimizer.param_groups: pg['lr'] = LEARNING_RATE[ep] # update learning rate\n",
    "        model.train()\n",
    "        trainloss, evalloss = [], []\n",
    "        for sxr, em in train_dl:\n",
    "            optimizer.zero_grad() # zero gradients\n",
    "            em_pred = model(sxr) # forward pass\n",
    "            loss = loss_fn(em_pred, em) # mean squared error loss on em\n",
    "            loss.backward() # backprop\n",
    "            optimizer.step() # update weights\n",
    "            trainloss.append((loss.item())) # save batch losses\n",
    "        model.eval() # evaluation mode\n",
    "        with torch.no_grad():\n",
    "            for sxr, em in val_dl:\n",
    "                em_pred = model(sxr)\n",
    "                loss = loss_fn(em_pred, em)\n",
    "                evalloss.append((loss.item()))\n",
    "        tloss_tot = sum(trainloss)/len(trainloss) # average train loss\n",
    "        eloss_tot = sum(evalloss)/len(evalloss) # average eval loss\n",
    "        # save model if improved        \n",
    "        if eloss_tot <= min(elog_tot, default=eloss_tot): \n",
    "            torch.save(model.state_dict(), f\"{SAVE_DIR}/mg_planet_tot.pth\"); endp=\" *\\n\"\n",
    "        else: endp = \"\\n\"\n",
    "        tlog_tot.append(tloss_tot) \n",
    "        elog_tot.append(eloss_tot)\n",
    "        print(f\"{ep+1}/{EPOCHS}: \"\n",
    "            f\"Eval: loss {eloss_tot:.4f} | \" + \n",
    "            f\"lr:{LEARNING_RATE[ep]:.1e} | \" + \n",
    "            f\"{time()-epoch_time:.0f}s, eta:{(time()-start_time)*(EPOCHS-ep)/(ep+1)/60:.0f}m |\", end=endp,  flush=True)\n",
    "        if ep >= 10 and eloss_tot > 9.0: return False, () # stop training, if not converging, try again\n",
    "    print(f\"Training time: {(time()-start_time)/60:.0f}mins\")\n",
    "    print(f\"Best losses: tot {min(elog_tot):.4f}\")\n",
    "    for l, n in zip([tlog_tot], [\"tot\"]): np.save(f\"{SAVE_DIR}/train_{n}_losses.npy\", l) # save losses\n",
    "    for l, n in zip([elog_tot], [\"tot\"]): np.save(f\"{SAVE_DIR}/eval_{n}_losses.npy\", l) # save losses\n",
    "    return True, (tlog_tot, elog_tot)\n",
    "\n",
    "# train the model (multiple attempts)\n",
    "for i in range(10): \n",
    "    success, logs = train()\n",
    "    if success: tlog_tot, elog_tot = logs; break\n",
    "    else: print(f\"Convergence failed, retrying... {i+1}/10\")\n",
    "assert success, \"Training failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 3))\n",
    "ce, ct = \"yellow\", \"red\"\n",
    "lw = 1.0\n",
    "ax[0].set_title(\"TOT Loss\")\n",
    "ax[0].plot(tlog_tot, color=ct, label=\"train\", linewidth=lw)\n",
    "ax[0].plot(elog_tot, color=ce, label=\"eval\", linewidth=lw)\n",
    "\n",
    "#now the same but with log scale\n",
    "ax[1].set_title(\"TOT Loss (log)\")\n",
    "ax[1].plot(tlog_tot, color=ct, label=\"train\", linewidth=lw)\n",
    "ax[1].plot(elog_tot, color=ce, label=\"eval\", linewidth=lw)\n",
    "ax[1].set_yscale(\"log\")\n",
    "ax[1].grid(True, which=\"both\", axis=\"y\")\n",
    "\n",
    "for a in ax.flatten(): a.legend(); a.set_xlabel(\"Epoch\"); a.set_ylabel(\"Loss\")\n",
    "plt.tight_layout()\n",
    "plt.show() if HAS_SCREEN else plt.savefig(f\"mg_data/{JOBID}/losses.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing network output\n",
    "for titl, best_model_path in zip([\"TOT\"], [\"mg_planet_tot.pth\"]):\n",
    "    model = SXRNet()\n",
    "    model.load_state_dict(torch.load(f\"{SAVE_DIR}/{best_model_path}\"))\n",
    "    model.eval()\n",
    "    ds = SXRDataset(EVAL_DS_PATH)\n",
    "    # ds = SXRDataset(TRAIN_DS_PATH)\n",
    "    rr, zz = ds.rr, ds.zz # grid coordinates\n",
    "    os.makedirs(f\"mg_data/{JOBID}/imgs\", exist_ok=True)\n",
    "    N_PLOTS = 12 if HAS_SCREEN else 50\n",
    "    for i in np.random.randint(0, len(ds), N_PLOTS):  \n",
    "        fig, axs = plt.subplots(1, 5, figsize=(20, 4))\n",
    "        sxr, em_ds = ds[i]\n",
    "        sxr, em_ds = sxr.to('cpu'), em_ds.to('cpu')\n",
    "        sxr, em_ds = sxr.view(1,-1), em_ds.view(1,1,GRID_SIZE,GRID_SIZE)\n",
    "        em_pred = model(sxr)\n",
    "        \n",
    "        em_pred = em_pred.detach().numpy().reshape(GRID_SIZE, GRID_SIZE)\n",
    "        em_ds = em_ds.detach().numpy().reshape(GRID_SIZE, GRID_SIZE)\n",
    "        bmin, bmax = np.min([em_ds, em_pred]), np.max([em_ds, em_pred]) # min max em\n",
    "        blevels = np.linspace(bmin, bmax, 13, endpoint=True)\n",
    "        em_mse = (em_ds - em_pred)**2\n",
    "        mse_levels1 = np.linspace(0, 0.5, 13, endpoint=True)\n",
    "        mse_levels2 = np.linspace(0, 0.05, 13, endpoint=True)\n",
    "\n",
    "        im00 = axs[0].contourf(rr, zz, em_ds, blevels, cmap=\"inferno\")\n",
    "        axs[0].set_title(\"Actual\")\n",
    "        axs[0].set_aspect('equal')\n",
    "        axs[0].set_ylabel(\"em\")\n",
    "        fig.colorbar(im00, ax=axs[0]) \n",
    "        im01 = axs[1].contourf(rr, zz, em_pred, blevels, cmap=\"inferno\")\n",
    "        axs[1].set_title(\"Predicted\")\n",
    "        fig.colorbar(im01, ax=axs[1])\n",
    "        im02 = axs[2].contour(rr, zz, em_ds, blevels, linestyles='dashed', cmap=\"inferno\")\n",
    "        axs[2].contour(rr, zz, em_pred, blevels, cmap=\"inferno\")\n",
    "        axs[2].set_title(\"Contours\")\n",
    "        fig.colorbar(im02, ax=axs[2])\n",
    "        im03 = axs[3].contourf(rr, zz, np.clip(em_mse, 0, 0.5), mse_levels1, cmap=\"inferno\")\n",
    "        axs[3].set_title(\"MSE 0.5\")\n",
    "        fig.colorbar(im03, ax=axs[3])\n",
    "        im04 = axs[4].contourf(rr, zz, np.clip(em_mse, 0.00001, 0.04999), mse_levels2, cmap=\"inferno\")\n",
    "        axs[4].set_title(\"MSE 0.05\")\n",
    "        fig.colorbar(im04, ax=axs[4])\n",
    "        for ax in axs.flatten(): ax.grid(False), ax.set_xticks([]), ax.set_yticks([]), ax.set_aspect(\"equal\")\n",
    "\n",
    "        #suptitle\n",
    "        plt.suptitle(f\"SXRNet: {titl} {i}\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        os.makedirs(f\"{SAVE_DIR}/imgs\", exist_ok=True)\n",
    "        plt.show() if HAS_SCREEN else plt.savefig(f\"{SAVE_DIR}/imgs/planet_{titl}_{i}.png\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test inference speed\n",
    "model = SXRNet()\n",
    "model.load_state_dict(torch.load(f\"{SAVE_DIR}/{best_model_path}\"))\n",
    "model.eval()\n",
    "ds = SXRDataset(EVAL_DS_PATH)\n",
    "n_samples = 100\n",
    "random_idxs = np.random.choice(n_samples, len(ds))\n",
    "#cpu\n",
    "cpu_times = []\n",
    "for i in random_idxs:\n",
    "    start_t = time()\n",
    "    sxr, em_ds = ds[i]\n",
    "    sxr, em_ds = sxr.to('cpu'), em_ds.to('cpu')\n",
    "    sxr, em_ds = sxr.view(1,-1), em_ds.view(1,1, GRID_SIZE, GRID_SIZE)\n",
    "    em_pred = model(sxr)\n",
    "    end_t = time()\n",
    "    cpu_times.append(end_t - start_t) \n",
    "# DEV\n",
    "model.to(DEV)\n",
    "dev_times = []\n",
    "for i in random_idxs:\n",
    "    sxr, em_ds = ds[i]\n",
    "    sxr, em_ds = sxr.view(1,-1), em_ds.view(1,1, GRID_SIZE, GRID_SIZE)\n",
    "    start_t = time()\n",
    "    em_pred = model(sxr)\n",
    "    end_t = time()\n",
    "    dev_times.append(end_t - start_t)    \n",
    "cpu_times, dev_times = np.array(cpu_times), np.array(dev_times)\n",
    "print(f\"cpu: inference time: {cpu_times.mean():.5f}s, std: {cpu_times.std():.5f}\")\n",
    "print(f\"dev: inference time: {dev_times.mean():.5f}s, std: {dev_times.std():.5f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
