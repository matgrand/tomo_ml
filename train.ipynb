{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from common import GSIZE, ISIZE, DS_SXR_SPLITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "N_DS = 50_000 # number of samples in the dataset\n",
    "# N_DS = 10_000 # number of samples in the dataset\n",
    "EPOCHS = 10 # 10\n",
    "BATCH_SIZE = 10 #10 \n",
    "LEARNING_RATE = np.ones(EPOCHS) * 3e-4 # learning rate\n",
    "# LEARNING_RATE = 3e-4*np.logspace(0, -2, EPOCHS)\n",
    "\n",
    "USE_REAL_DS = True # use real dataset\n",
    "# USE_REAL_DS = False\n",
    "\n",
    "RECALC_SXR = False # recalculate SXR signals\n",
    "# RECALC_SXR = True\n",
    "\n",
    "# architecture\n",
    "# ARCHITECTURE = SXRNetU32\n",
    "# ARCHITECTURE = SXRNetU32Big\n",
    "# ARCHITECTURE = SXRNetU55\n",
    "# ARCHITECTURE = SXRNetU110\n",
    "# ARCHITECTURE = SXRNetU64\n",
    "ARCHITECTURE = SXRNetLinear1\n",
    "# ARCHITECTURE = SXRNetLinear2\n",
    "# ARCHITECTURE = SXRNetLnCos\n",
    "\n",
    "# NOISE_LEVEL = 0.03 #0.02 # noise level [fraction on the mean]\n",
    "NOISE_LEVEL = 0.0\n",
    "# RANDOM_REMOVE = 3 #3 # number of random sensors to remove each time\n",
    "RANDOM_REMOVE = 0\n",
    "\n",
    "N_PLOTS = 10 if HAS_SCREEN else 50\n",
    "SAVE_PATH = SAVE_DIR + \"/mg_tomo.pth\" # save model path\n",
    "\n",
    "# LOAD_PRETRAINED = SAVE_DIR + \"/mg_tomo_best.pth\" # best pretrained model path\n",
    "# LOAD_PRETRAINED = SAVE_DIR + \"/mg_tomo.pth\" # keep training the same model\n",
    "LOAD_PRETRAINED = None # pretrained model path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataset\n",
    "ds = SXRDataset(N_DS//10, GSIZE, USE_REAL_DS, NOISE_LEVEL, RANDOM_REMOVE, RECALC_SXR)\n",
    "print(f\"Dataset length: {len(ds)}\")\n",
    "print(f\"Input shape: {ds[0][0].shape}\")\n",
    "print(f\"Output shape: {ds[0][1].shape}\")\n",
    "ds.show_examples(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model\n",
    "model = ARCHITECTURE(ISIZE, GSIZE)\n",
    "input = torch.randn(1, ISIZE)\n",
    "output = model(input)\n",
    "print(f'Input: {input.shape} Output: {output.shape}')\n",
    "assert output.shape == (1, GSIZE, GSIZE), f'Bad output shape: {output.shape}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "def train():\n",
    "    train_ds = SXRDataset(N_DS, GSIZE, USE_REAL_DS, NOISE_LEVEL, RANDOM_REMOVE, RECALC_SXR)\n",
    "    val_ds = SXRDataset(N_DS//10, GSIZE, USE_REAL_DS, NOISE_LEVEL, RANDOM_REMOVE, RECALC_SXR)\n",
    "    train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True) # initialize DataLoader\n",
    "    val_dl = DataLoader(val_ds, batch_size=1, shuffle=False)  \n",
    "    model = ARCHITECTURE(ISIZE, GSIZE)  # instantiate model\n",
    "    if LOAD_PRETRAINED is not None: # load pretrained model\n",
    "        try:\n",
    "            model.load_state_dict(torch.load(LOAD_PRETRAINED, map_location=torch.device(\"cpu\"))) # load pretrained model\n",
    "            print(f\"Pretrained model loaded: {LOAD_PRETRAINED}\")\n",
    "            torch.save(model.state_dict(), SAVE_PATH)\n",
    "        except: print(f\"Failed to load pretrained model: {LOAD_PRETRAINED}\")\n",
    "    model.to(DEV) # move model to DEV\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE[0])\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE[0], momentum=0.9)\n",
    "    loss_fn = torch.nn.MSELoss() # loss function\n",
    "    tlog_tot, elog_tot = [], []# logs for losses\n",
    "    start_time = time() # start time\n",
    "    for ep in range(EPOCHS): # epochs\n",
    "        epoch_time = time()\n",
    "        for pg in optimizer.param_groups: pg['lr'] = LEARNING_RATE[ep] # update learning rate \n",
    "        model.train()\n",
    "        trainloss, evalloss = torch.zeros(len(train_dl)), torch.zeros(len(val_dl)) # initialize losses\n",
    "        # batches = tqdm(train_dl, desc=f\"Epoch {ep+1}/{EPOCHS}\", leave=False) if HAS_SCREEN else train_dl\n",
    "        batches = train_dl\n",
    "        for bi, (sxr, em) in enumerate(batches):\n",
    "            optimizer.zero_grad() # zero gradients\n",
    "            em_pred = model(sxr) # forward pass\n",
    "            assert em_pred.shape == em.shape, f\"Shapes do not match: {em_pred.shape} != {em.shape}\"\n",
    "            loss = loss_fn(em_pred, em) # mean squared error loss on em\n",
    "            loss.backward() # backprop\n",
    "            optimizer.step() # update weights\n",
    "            trainloss[bi] = loss.item() # save loss\n",
    "        # evaluation\n",
    "        model.eval() # evaluation mode\n",
    "        with torch.no_grad():\n",
    "            for bi, (sxr, em) in enumerate(val_dl):\n",
    "                em_pred = model(sxr)\n",
    "                loss = loss_fn(em_pred, em)\n",
    "                evalloss[bi] = loss.item()\n",
    "        tloss_tot = trainloss.mean().item() # total training loss\n",
    "        eloss_tot = evalloss.mean().item() # total evaluation loss\n",
    "        # save model if improved        \n",
    "        if eloss_tot <= min(elog_tot, default=eloss_tot): \n",
    "            torch.save(model.state_dict(), SAVE_PATH); endp=\" *\\n\"\n",
    "        else: endp = \"\\n\"\n",
    "        tlog_tot.append(tloss_tot) \n",
    "        elog_tot.append(eloss_tot)\n",
    "        print(f\"{ep+1}/{EPOCHS}: \"\n",
    "            f\"Eval: loss {sqrt(eloss_tot):.6f} ({eloss_tot:.1e})| \" + # scale back to original units\n",
    "            f\"lr:{LEARNING_RATE[ep]:.1e} | \" +\n",
    "            f\"{time()-epoch_time:.0f}s, eta:{(time()-start_time)*(EPOCHS-ep)/(ep+1)/60:.0f}m |\", end=endp,  flush=True)\n",
    "        # if ep >= 10 and eloss_tot > 8e-6: return False, () # stop training, if not converging, try again\n",
    "    print(f\"Training time: {(time()-start_time)/60:.0f}mins\")\n",
    "    print(f\"Best losses: tot {min(elog_tot):.4f}\")\n",
    "    for l, n in zip([tlog_tot], [\"tot\"]): np.save(f\"{SAVE_DIR}/train_{n}_losses.npy\", l) # save losses\n",
    "    for l, n in zip([elog_tot], [\"tot\"]): np.save(f\"{SAVE_DIR}/eval_{n}_losses.npy\", l) # save losses\n",
    "    return True, (tlog_tot, elog_tot)\n",
    "\n",
    "# train the model (multiple attempts)\n",
    "for i in range(10): \n",
    "    success, logs = train()\n",
    "    if success: tlog_tot, elog_tot = logs; break\n",
    "    else: print(f\"Convergence failed, retrying... {i+1}/10\")\n",
    "assert success, \"Training failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 3))\n",
    "ce, ct = \"yellow\", \"red\"\n",
    "lw = 1.0\n",
    "ax[0].set_title(\"TOT Loss\")\n",
    "ax[0].plot(tlog_tot, color=ct, label=\"train\", linewidth=lw)\n",
    "ax[0].plot(elog_tot, color=ce, label=\"eval\", linewidth=lw)\n",
    "\n",
    "#now the same but with log scale\n",
    "ax[1].set_title(\"TOT Loss (log)\")\n",
    "ax[1].plot(tlog_tot, color=ct, label=\"train\", linewidth=lw)\n",
    "ax[1].plot(elog_tot, color=ce, label=\"eval\", linewidth=lw)\n",
    "ax[1].set_yscale(\"log\")\n",
    "ax[1].grid(True, which=\"both\", axis=\"y\")\n",
    "\n",
    "for a in ax.flatten(): a.legend(); a.set_xlabel(\"Epoch\"); a.set_ylabel(\"Loss\")\n",
    "plt.tight_layout()\n",
    "plt.show() if HAS_SCREEN else plt.savefig(f\"mg_data/{JOBID}/losses.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing clean data network output\n",
    "model = ARCHITECTURE(ISIZE, GSIZE)\n",
    "model.load_state_dict(torch.load(SAVE_PATH, map_location=torch.device(\"cpu\")))\n",
    "model.eval()\n",
    "ds = SXRDataset(N_DS//10, GSIZE, USE_REAL_DS, 0.0, 0, RECALC_SXR)\n",
    "rr, zz = ds.RR, ds.ZZ # grid coordinates\n",
    "vdi0, vdi1, vdc0, vdc1, vde0, vde1, hor0, hor1 = DS_SXR_SPLITS[0], DS_SXR_SPLITS[1], DS_SXR_SPLITS[1], DS_SXR_SPLITS[2], DS_SXR_SPLITS[2], DS_SXR_SPLITS[3], DS_SXR_SPLITS[3], DS_SXR_SPLITS[4]\n",
    "for i in np.random.randint(0, len(ds), N_PLOTS):  \n",
    "    sxr, em_ds = ds[i]\n",
    "    em = em_ds.detach().cpu().numpy().reshape(GSIZE, GSIZE)\n",
    "    sxr, em_ds = sxr.to('cpu'), em_ds.to('cpu')\n",
    "    sxr, em_ds = sxr.view(1,-1), em_ds.view(1,1,GSIZE,GSIZE)\n",
    "    # clean data\n",
    "    em_pred = model(sxr).detach().numpy().reshape(GSIZE, GSIZE)\n",
    "    vdi, vdc, vde, hor = sxr[0,vdi0:vdi1], sxr[0,vdc0:vdc1], sxr[0,vde0:vde1], sxr[0,hor0:hor1]\n",
    "    plot_net_example(em, em_pred, [vdi, vdc, vde, hor], rr, zz, f\"CLEAN {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # testing noisy network output\n",
    "# model = ARCHITECTURE(ISIZE, GSIZE)\n",
    "# model.load_state_dict(torch.load(SAVE_PATH, map_location=torch.device(\"cpu\")))\n",
    "# model.eval()\n",
    "# ds = SXRDataset(N_DS//10, GSIZE, USE_REAL_DS, 0.02, 0, RECALC_SXR)\n",
    "# rr, zz = ds.RR, ds.ZZ # grid coordinates\n",
    "# vdi0, vdi1, vdc0, vdc1, vde0, vde1, hor0, hor1 = DS_SXR_SPLITS[0], DS_SXR_SPLITS[1], DS_SXR_SPLITS[1], DS_SXR_SPLITS[2], DS_SXR_SPLITS[2], DS_SXR_SPLITS[3], DS_SXR_SPLITS[3], DS_SXR_SPLITS[4]\n",
    "# for i in np.random.randint(0, len(ds), N_PLOTS):  \n",
    "#     sxr, em_ds = ds[i]\n",
    "#     em = em_ds.detach().cpu().numpy().reshape(GSIZE, GSIZE)\n",
    "#     sxr, em_ds = sxr.to('cpu'), em_ds.to('cpu')\n",
    "#     sxr, em_ds = sxr.view(1,-1), em_ds.view(1,1,GSIZE,GSIZE)\n",
    "#     # clean data\n",
    "#     em_pred = model(sxr).detach().numpy().reshape(GSIZE, GSIZE)\n",
    "#     vdi, vdc, vde, hor = sxr[0,vdi0:vdi1], sxr[0,vdc0:vdc1], sxr[0,vde0:vde1], sxr[0,hor0:hor1]\n",
    "#     plot_net_example(em, em_pred, [vdi, vdc, vde, hor], rr, zz, f\"CLEAN {i}\")\n",
    "#     # noisy data\n",
    "#     noise = torch.randn_like(sxr) * 0.02 * torch.max(sxr)\n",
    "#     noisy_sxr = sxr + noise\n",
    "#     em_pred = model(noisy_sxr).detach().numpy().reshape(GSIZE, GSIZE)\n",
    "#     vdi, vdc, vde, hor = noisy_sxr[0,vdi0:vdi1], noisy_sxr[0,vdc0:vdc1], noisy_sxr[0,vde0:vde1], noisy_sxr[0,hor0:hor1]\n",
    "#     plot_net_example(em, em_pred, [vdi, vdc, vde, hor], rr, zz, f\"NOISY {i}\")\n",
    "#     # noisy data with random sensors removed\n",
    "#     random_remove_n = 4\n",
    "#     noisy_sxr[0, np.random.randint(0, ISIZE, random_remove_n)] = 0 # remove some sensors\n",
    "#     em_pred = model(noisy_sxr).detach().numpy().reshape(GSIZE, GSIZE)\n",
    "#     vdi, vdc, vde, hor = noisy_sxr[0,vdi0:vdi1], noisy_sxr[0,vdc0:vdc1], noisy_sxr[0,vde0:vde1], noisy_sxr[0,hor0:hor1]\n",
    "#     plot_net_example(em, em_pred, [vdi, vdc, vde, hor], rr, zz, f\"NOISY-REMOVED {i}\")\n",
    "#     print(f\"------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on REAL data\n",
    "model = ARCHITECTURE(ISIZE, GSIZE)\n",
    "model.load_state_dict(torch.load(SAVE_PATH, map_location=torch.device(\"cpu\")))\n",
    "model.eval()\n",
    "ds = SXRDataset(N_DS//10, GSIZE, True, 0.0, 0, calc_sxr=False)\n",
    "# ds = SXRDataset(N_DS//10, GSIZE, True, 0.0, 0, calc_sxr=True)\n",
    "rr, zz = ds.RR, ds.ZZ # grid coordinates\n",
    "vdi0, vdi1, vdc0, vdc1, vde0, vde1, hor0, hor1 = DS_SXR_SPLITS[0], DS_SXR_SPLITS[1], DS_SXR_SPLITS[1], DS_SXR_SPLITS[2], DS_SXR_SPLITS[2], DS_SXR_SPLITS[3], DS_SXR_SPLITS[3], DS_SXR_SPLITS[4]\n",
    "for i in np.random.randint(0, len(ds), N_PLOTS):  \n",
    "    sxr, em_ds = ds[i]\n",
    "    em = em_ds.detach().cpu().numpy().reshape(GSIZE, GSIZE)\n",
    "    sxr, em_ds = sxr.to('cpu'), em_ds.to('cpu')\n",
    "    sxr, em_ds = sxr.view(1,-1), em_ds.view(1,1,GSIZE,GSIZE)\n",
    "    # clean data\n",
    "    em_pred = model(sxr).detach().numpy().reshape(GSIZE, GSIZE)\n",
    "    vdi, vdc, vde, hor = sxr[0,vdi0:vdi1], sxr[0,vdc0:vdc1], sxr[0,vde0:vde1], sxr[0,hor0:hor1]\n",
    "    plot_net_example(em, em_pred, [vdi, vdc, vde, hor], rr, zz, f\"Real {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on SIM data\n",
    "model = ARCHITECTURE(ISIZE, GSIZE)\n",
    "model.load_state_dict(torch.load(SAVE_PATH, map_location=torch.device(\"cpu\")))\n",
    "model.eval()\n",
    "ds = SXRDataset(N_DS//10, GSIZE, real=False, noise_level=0.0, random_remove=0, calc_sxr=False)\n",
    "rr, zz = ds.RR, ds.ZZ # grid coordinates\n",
    "vdi0, vdi1, vdc0, vdc1, vde0, vde1, hor0, hor1 = DS_SXR_SPLITS[0], DS_SXR_SPLITS[1], DS_SXR_SPLITS[1], DS_SXR_SPLITS[2], DS_SXR_SPLITS[2], DS_SXR_SPLITS[3], DS_SXR_SPLITS[3], DS_SXR_SPLITS[4]\n",
    "for i in np.random.randint(0, len(ds), N_PLOTS):  \n",
    "    sxr, em_ds = ds[i]\n",
    "    em = em_ds.detach().cpu().numpy().reshape(GSIZE, GSIZE)\n",
    "    sxr, em_ds = sxr.to('cpu'), em_ds.to('cpu')\n",
    "    sxr, em_ds = sxr.view(1,-1), em_ds.view(1,1,GSIZE,GSIZE)\n",
    "    # clean data\n",
    "    em_pred = model(sxr).detach().numpy().reshape(GSIZE, GSIZE)\n",
    "    vdi, vdc, vde, hor = sxr[0,vdi0:vdi1], sxr[0,vdc0:vdc1], sxr[0,vde0:vde1], sxr[0,hor0:hor1]\n",
    "    plot_net_example(em, em_pred, [vdi, vdc, vde, hor], rr, zz, f\"Simulated {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test inference speed\n",
    "model = ARCHITECTURE(ISIZE, GSIZE)\n",
    "model.load_state_dict(torch.load(SAVE_PATH, map_location=torch.device(\"cpu\")))\n",
    "model.eval()\n",
    "ds = SXRDataset(N_DS//10, GSIZE, USE_REAL_DS, 0.0)\n",
    "n_samples = 100\n",
    "random_idxs = np.random.choice(n_samples, len(ds))\n",
    "#cpu\n",
    "cpu_times = []\n",
    "for i in random_idxs:\n",
    "    start_t = time()\n",
    "    sxr, em_ds = ds[i]\n",
    "    sxr, em_ds = sxr.to('cpu'), em_ds.to('cpu')\n",
    "    sxr, em_ds = sxr.view(1,-1), em_ds.view(1,1, GSIZE, GSIZE)\n",
    "    em_pred = model(sxr)\n",
    "    end_t = time()\n",
    "    cpu_times.append(end_t - start_t) \n",
    "# DEV\n",
    "model.to(DEV)\n",
    "dev_times = []\n",
    "for i in random_idxs:\n",
    "    sxr, em_ds = ds[i]\n",
    "    sxr, em_ds = sxr.view(1,-1), em_ds.view(1,1, GSIZE, GSIZE)\n",
    "    start_t = time()\n",
    "    em_pred = model(sxr)\n",
    "    end_t = time()\n",
    "    dev_times.append(end_t - start_t)    \n",
    "cpu_times, dev_times = np.array(cpu_times), np.array(dev_times)\n",
    "print(f\"cpu: inference time: {1000*cpu_times.mean():.3f}[ms], std: {1000*cpu_times.std():.3f}[ms]\")\n",
    "print(f\"dev: inference time: {1000*dev_times.mean():.3f}[ms], std: {1000*dev_times.std():.3f}[ms]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
